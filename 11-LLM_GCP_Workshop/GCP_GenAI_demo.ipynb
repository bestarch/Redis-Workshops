{
  "cells": [
    {
      "cell_type": "code",
      "id": "HqRohfRRZgFWtj1zYJIyvR5n",
      "metadata": {
        "tags": [],
        "id": "HqRohfRRZgFWtj1zYJIyvR5n"
      },
      "source": [
        "!pwd\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Install required libraries\n",
        "!python3 -m pip -q install redis\n",
        "!pip install -U langchain gradio\n",
        "!pip install langchain-community \"unstructured[pdf]\"\n",
        "!pip install -U langchain-google-vertexai\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment & execute the following code in case Redis Enterprise is not available\n",
        "##################################################################################\n",
        "\n",
        "# %%sh\n",
        "# curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "# echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "# sudo apt-get update  > /dev/null 2>&1\n",
        "# sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "# redis-stack-server --daemonize yes"
      ],
      "metadata": {
        "id": "1zTK0KXJUwzX"
      },
      "id": "1zTK0KXJUwzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Update the 'host' field with the correct Redis host URL\n",
        "host = 'redis-17001.c330.asia-south1-1.gce.redns.redis-cloud.com'\n",
        "port = 17001\n",
        "password = 'admin'\n",
        "requirePass = True\n",
        "\n",
        "## For redis-stack-server, comment out the above code and uncomment the following:\n",
        "# host = 'localhost'\n",
        "# requirePass = False"
      ],
      "metadata": {
        "id": "sjUooyI9VlAu"
      },
      "id": "sjUooyI9VlAu",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "if requirePass:\n",
        "    client = redis.Redis(host = host, port=port, decode_responses=True, password=password)\n",
        "else:\n",
        "    client = redis.Redis(host = 'localhost', decode_responses=True)\n",
        "\n",
        "print(client.ping())\n",
        "# Clear Redis database (optional)\n",
        "client.flushdb()\n",
        "\n",
        "REDIS_URL = f\"redis://:{password}@{host}:{port}\"\n",
        "INDEX_NAME = f\"idx_qna\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UnZUjIFVxWA",
        "outputId": "cf4fe1c0-2980-45fd-d0d0-8325f9bc9f94"
      },
      "id": "4UnZUjIFVxWA",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Authenticate with GCP & set project id and region\n",
        "from google.colab import auth\n",
        "from getpass import getpass\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "# input your GCP project ID and region for Vertex AI\n",
        "PROJECT_ID = 'central-beach-194106' #getpass(\"PROJECT_ID:\")\n",
        "REGION = 'asia-south1' #input(\"REGION:\")\n",
        "\n",
        "print(f'PROJECT_ID: {PROJECT_ID} & REGION: {REGION}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_38Ye7niWPZR",
        "outputId": "4484d3c8-6a3a-4562-95ab-739d4edd235b"
      },
      "id": "_38Ye7niWPZR",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n",
            "PROJECT_ID: central-beach-194106 & REGION: asia-south1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/abhi-data-2024/how_india_shops_online.pdf -O report.pdf\n"
      ],
      "metadata": {
        "id": "cnEl0UcxWV5w"
      },
      "id": "cnEl0UcxWV5w",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "\n",
        "doc = \"report.pdf\"\n",
        "\n",
        "# set up the file loader/extractor and text splitter to create chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500, chunk_overlap=100, add_start_index=True\n",
        ")\n",
        "loader = UnstructuredPDFLoader(doc)\n",
        "\n",
        "# extract, load, and make chunks\n",
        "chunks = loader.load_and_split(text_splitter)\n",
        "print(\"Preprocessing completed and created\", len(chunks), \"chunks of the original pdf\", doc)\n"
      ],
      "metadata": {
        "id": "-B_EHwP7ouXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9bc7d9-7fc0-4b4a-c6fb-bb7a83b7a5e6"
      },
      "id": "-B_EHwP7ouXf",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done preprocessing. Created 42 chunks of the original pdf how_india_shops_online.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create text embeddings with Vertex AI embedding model\n",
        "\n",
        "Use the Vertex AI API for text embeddings, developed by Google.\n",
        "\n",
        "Text embeddings are a dense vector representation of a piece of content such that, if two pieces of content are semantically similar, their respective embeddings are located near each other in the embedding vector space. This representation can be used to solve common NLP tasks, such as:\n",
        "\n",
        "\n",
        "*   Semantic search: Search text ranked by semantic similarity.\n",
        "*   Recommendation: Return items with text attributes similar to the given text.\n",
        "*   Classification: Return the class of items whose text attributes are similar to the given text.\n",
        "*   Clustering: Cluster items whose text attributes are similar to the given text.\n",
        "*   Outlier Detection: Return items where text attributes are least related to the given text.\n",
        "\n",
        "The Vertex AI text-embeddings API lets you create a text embedding using Generative AI on Vertex AI. The textembedding-gecko model accepts a maximum of 3,072 input tokens (i.e. words) and outputs 768-dimensional vector embeddings."
      ],
      "metadata": {
        "id": "hKHXwtSGwNPr"
      },
      "id": "hKHXwtSGwNPr"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.redis import Redis\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\", project=PROJECT_ID, location=REGION)\n",
        "\n",
        "def get_vectordb() -> Redis:\n",
        "    \"\"\"Create the Redis vectordb.\"\"\"\n",
        "\n",
        "    try:\n",
        "        vectordb = Redis.from_existing_index(\n",
        "            embedding=embeddings,\n",
        "            index_name=INDEX_NAME,\n",
        "            redis_url=REDIS_URL\n",
        "        )\n",
        "        return vectordb\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Load Redis with documents\n",
        "    vectordb = Redis.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "        redis_url=REDIS_URL\n",
        "    )\n",
        "    return vectordb\n",
        "\n",
        "\n",
        "redis = get_vectordb()\n",
        "\n",
        "#embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
      ],
      "metadata": {
        "id": "A6H_vHkrSxco"
      },
      "id": "A6H_vHkrSxco",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Include RAG\n",
        "\n",
        "We're going to build a complete RAG pipeline from scratch incorporating the following components:\n",
        "\n",
        "Standard retrieval and chat completion\n",
        "Dense content representation to improve accuracy\n",
        "Query re-writing to improve accuracy\n",
        "Semantic caching to improve performance\n",
        "Conversational session history to improve personalization"
      ],
      "metadata": {
        "id": "kVJdyD1OVYE-"
      },
      "id": "kVJdyD1OVYE-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Prompt template\n",
        "PromptTemplate defines the exect text of the response that would be fed to the LLM. This step is optional, but the defaults usually work well for OpenAI and might fall short for other models."
      ],
      "metadata": {
        "id": "aNtLTGcvR-IS"
      },
      "id": "aNtLTGcvR-IS"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to define prompt template\n",
        "\n",
        "def create_prompt():\n",
        "    \"\"\"Create the QA chain.\"\"\"\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import RetrievalQA\n",
        "\n",
        "    # Define our prompt\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    This should be in the following format:\n",
        "\n",
        "    Question: [question here]\n",
        "    Answer: [answer here]\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Context:\n",
        "    ---------\n",
        "    {context}\n",
        "    ---------\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "BCtEFE8ziT5d"
      },
      "id": "BCtEFE8ziT5d",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Invoke Google Vertex LLM using Langchain\n",
        "# This is where the Langchain brings all the components together in a form of a simple QnA chain\n",
        "from langchain_google_vertexai import VertexAI\n",
        "\n",
        "llm = VertexAI(\n",
        "    model_name=\"gemini-1.5-pro-preview-0409\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.5,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=redis.as_retriever(search_type=\"similarity_distance_threshold\",search_kwargs={\"distance_threshold\":0.5}),\n",
        "    #return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": create_prompt()},\n",
        "    #verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "DYtdUxxFSxh9"
      },
      "id": "DYtdUxxFSxh9",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run('As per the report, what are some relevant shopping habits of Indians?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "XelSUbTkS7rj",
        "outputId": "96b51516-fefb-4e80-b839-3b28e290b568"
      },
      "id": "XelSUbTkS7rj",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    Answer:\\n    * Indians from tier-2, 3, and 4 cities are driving a retail revolution in the country, contributing to more than 80% of sales for Meesho and Amazon during the festive season in 2023. \\n    * Urban dwellers engage in detailed comparison shopping, while consumers in the rest of India prioritize platform familiarity and delivery date, followed by offers and discounts. \\n    * Late millennials and Gen X consumers favor bulk purchases and prioritize discounts and deals on those bulk orders. \\n    * Early millennials in both urban and non-urban areas prefer monthly purchases for essentials and quick delivery for need-based products. \\n    * Gen Z consumers tend to make smaller value purchases, prioritizing quick delivery options, and showing less concern about additional delivery charges on smaller orders. \\n    * Payment options linked to discounts are a strong draw for consumers across all cohorts, regardless of location. \\n    * Loyalty programs offering immediate benefits and subscription services attract first-time consumers, especially in metros. \\n    * Consumers gravitate towards platforms with the widest selection and familiar interfaces due to the challenge of brand availability (particularly new-age D2C and niche brands). \\n    * Both urban dwellers and rest of India consumers prefer purchasing fresh produce and non-vegetarian grocery items through in-person or offline channels due to concerns about quality, lack of return policies, freshness issues, or a desire to support local businesses. \\n    * Snacks, bathing products, and other essentials are top-selling grocery items online due to minimal risk of low quality, quick delivery options, and the possibility of bulk orders. \\n    * Reviews, ratings, and photos from other customers heavily influence both urban and non-urban shoppers, providing confidence in their selections and showcasing products in real-life settings. \\n    * Product price is a significant factor for 77% of rest of India consumers in the fashion and accessories category, leading them to continue using their preferred platforms. \\n    * Marketplace apps are favored over category-based platforms among rest of India respondents, highlighting the importance of deals, discounts, accessibility, user-friendly interfaces, and platform familiarity. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def handle(query):\n",
        "    response = qa.run(query)\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(fn=handle, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "UmsYlwP9XZ2G",
        "outputId": "f9458ad5-f482-4f27-dc7e-140597bc1e4f"
      },
      "id": "UmsYlwP9XZ2G",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvmnrq_3YY6p",
        "outputId": "7cd49c01-6bf6-4448-a02f-7012c5f555de"
      },
      "id": "nvmnrq_3YY6p",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}