{
  "cells": [
    {
      "cell_type": "code",
      "id": "HqRohfRRZgFWtj1zYJIyvR5n",
      "metadata": {
        "tags": [],
        "id": "HqRohfRRZgFWtj1zYJIyvR5n"
      },
      "source": [
        "!pwd\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Install required libraries\n",
        "!python3 -m pip -q install redis\n",
        "!pip install -U langchain gradio\n",
        "!pip install -U langchain-core\n",
        "!pip install -U langchain-community\n",
        "!pip install -qU pypdf\n",
        "!pip install -U redisvl\n",
        "!pip install openai\n",
        "!pip install -qU langchain-openai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Update the 'host' field with the correct Redis host URL\n",
        "host = 'redis-12000.redis-poc.dlqueue.com'\n",
        "port = 12000\n",
        "password = 'admin'\n",
        "requirePass = True\n"
      ],
      "metadata": {
        "id": "sjUooyI9VlAu"
      },
      "id": "sjUooyI9VlAu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "if requirePass:\n",
        "    client = redis.Redis(host = host, port=port, decode_responses=True, password=password)\n",
        "else:\n",
        "    client = redis.Redis(host = 'localhost', decode_responses=True)\n",
        "\n",
        "print(client.ping())\n",
        "# Clear Redis database (optional)\n",
        "client.flushdb()\n",
        "\n",
        "REDIS_URL = f\"redis://:{password}@{host}:{port}\"\n",
        "INDEX_NAME = f\"idx_qna\""
      ],
      "metadata": {
        "id": "4UnZUjIFVxWA"
      },
      "id": "4UnZUjIFVxWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/abhi-data-2024/how_india_shops_online.pdf -O report.pdf\n"
      ],
      "metadata": {
        "id": "cnEl0UcxWV5w"
      },
      "id": "cnEl0UcxWV5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "#from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "file = \"report.pdf\"\n",
        "\n",
        "# set up the file loader/extractor and text splitter to create chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500, chunk_overlap=50, add_start_index=True\n",
        ")\n",
        "\n",
        "loader = PyPDFLoader(file)\n",
        "documents = loader.load()\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "#chunked_docs = [doc.page_content for doc in chunks]"
      ],
      "metadata": {
        "id": "-B_EHwP7ouXf"
      },
      "id": "-B_EHwP7ouXf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_docs = [doc.page_content for doc in chunks]\n",
        "print(chunks)\n",
        "print(\"*****\")\n",
        "print(chunked_docs)"
      ],
      "metadata": {
        "id": "YrUStfXjniIa"
      },
      "id": "YrUStfXjniIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "# setup the API Key\n",
        "api_key = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "FyHpEdbn0MlA"
      },
      "id": "FyHpEdbn0MlA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create text embeddings with Open AI embedding model\n",
        "\n",
        "Use the Open AI for text embeddings, developed by Google.\n",
        "\n",
        "Text embeddings are a dense vector representation of a p\\iece of content such that, if two pieces of content are semantically similar, their respective embeddings are located near each other in the embedding vector space. This representation can be used to solve common NLP tasks, such as:\n",
        "\n",
        "\n",
        "*   Semantic search: Search text ranked by semantic similarity.\n",
        "*   Recommendation: Return items with text attributes similar to the given text.\n",
        "*   Classification: Return the class of items whose text attributes are similar to the given text.\n",
        "*   Clustering: Cluster items whose text attributes are similar to the given text.\n",
        "*   Outlier Detection: Return items where text attributes are least related to the given text.\n",
        "\n",
        "The Open AI text-embeddings API lets you create a text embedding using Generative AI on Vertex AI. The text-embedding-3-large model accepts a maximum of 4096 input tokens (i.e. words) and outputs 1024-dimensional vector embeddings."
      ],
      "metadata": {
        "id": "hKHXwtSGwNPr"
      },
      "id": "hKHXwtSGwNPr"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.redis import Redis\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\",\n",
        "    dimensions=1024,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "def get_vectordb() -> Redis:\n",
        "    \"\"\"Create the Redis vectordb.\"\"\"\n",
        "    # Load Redis with documents\n",
        "    vectordb = Redis.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "        redis_url=REDIS_URL\n",
        "    )\n",
        "    return vectordb\n",
        "\n",
        "\n",
        "redis = get_vectordb()\n"
      ],
      "metadata": {
        "id": "A6H_vHkrSxco"
      },
      "id": "A6H_vHkrSxco",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Include RAG\n",
        "\n",
        "We're going to build a complete RAG pipeline from scratch incorporating the following components:\n",
        "\n",
        "Standard retrieval and chat completion\n",
        "Dense content representation to improve accuracy\n",
        "Query re-writing to improve accuracy\n",
        "Semantic caching to improve performance\n",
        "Conversational session history to improve personalization"
      ],
      "metadata": {
        "id": "kVJdyD1OVYE-"
      },
      "id": "kVJdyD1OVYE-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Prompt template\n",
        "PromptTemplate defines the exect text of the response that would be fed to the LLM. This step is optional, but the defaults usually work well for OpenAI and might fall short for other models."
      ],
      "metadata": {
        "id": "aNtLTGcvR-IS"
      },
      "id": "aNtLTGcvR-IS"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to define prompt template\n",
        "\n",
        "def create_prompt():\n",
        "    \"\"\"Create the QA chain.\"\"\"\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import RetrievalQA\n",
        "\n",
        "    # Define our prompt\n",
        "    prompt_template = \"\"\"Use only the following pieces of context to answer the question. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    This should be in the following format:\n",
        "\n",
        "    Question: [question here]\n",
        "    Answer: [answer here]\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Context:\n",
        "    ---------\n",
        "    {context}\n",
        "    ---------\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "BCtEFE8ziT5d"
      },
      "id": "BCtEFE8ziT5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.5,\n",
        "    max_retries=2,\n",
        "    api_key=api_key,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=redis.as_retriever(search_type=\"similarity_distance_threshold\",search_kwargs={\"distance_threshold\":0.5}),\n",
        "    #return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": create_prompt()},\n",
        "    #verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "Hru_djYR3eAf"
      },
      "id": "Hru_djYR3eAf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are some motivations for shopping online?')['result']"
      ],
      "metadata": {
        "id": "XelSUbTkS7rj"
      },
      "id": "XelSUbTkS7rj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('How do Indians like to pay for shopping online?')['result']"
      ],
      "metadata": {
        "id": "B2f9CC3JI2BL"
      },
      "id": "B2f9CC3JI2BL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are some known challenges in shopping online?')['result']"
      ],
      "metadata": {
        "id": "LLHoWVshJJHK"
      },
      "id": "LLHoWVshJJHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('How home and kitchen segment is growing?')['result']"
      ],
      "metadata": {
        "id": "5PVFYBW1NNKm"
      },
      "id": "5PVFYBW1NNKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are the effects of social media on online shopping?')['result']"
      ],
      "metadata": {
        "id": "bgfyRlzCNquj"
      },
      "id": "bgfyRlzCNquj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are some relevant items that are shopped online?')['result']"
      ],
      "metadata": {
        "id": "xClDk__XHFw-"
      },
      "id": "xClDk__XHFw-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def handle(query):\n",
        "    response = qa.run(query)\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(fn=handle, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "UmsYlwP9XZ2G"
      },
      "id": "UmsYlwP9XZ2G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "id": "nvmnrq_3YY6p"
      },
      "id": "nvmnrq_3YY6p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}